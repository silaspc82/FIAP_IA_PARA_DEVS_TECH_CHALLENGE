{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silaspc82/FIAP_IA_PARA_DEVS_TECH_CHALLENGE/blob/main/Tech_Challenge_Fase_04/FIAP_TechCallenge_Fase_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZtdRu-ckzt2",
        "outputId": "cfca6683-1030-43a3-ead7-e673c1d7cf04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless deepface tf-keras tqdm deepface mediapipe face-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-21DUVIckd6C",
        "outputId": "3429cf2d-04fd-459c-839a-6505c88d1b40"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: deepface in /usr/local/lib/python3.10/dist-packages (0.0.93)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: face-recognition in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (11.0.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.10.0.84)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.17.1)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.5.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (3.0.3)\n",
            "Requirement already satisfied: flask-cors>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (5.0.0)\n",
            "Requirement already satisfied: mtcnn>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.0.0)\n",
            "Requirement already satisfied: retina-face>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (0.0.17)\n",
            "Requirement already satisfied: fire>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (0.7.0)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (23.0.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from face-recognition) (0.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face-recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face-recognition) (19.24.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.16.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.10/dist-packages (from mtcnn>=0.1.0->deepface) (4.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->deepface) (2024.8.30)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"
      ],
      "metadata": {
        "id": "ozyhTP8rXdye"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pBKTdHjpij0y"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe import solutions\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from deepface import DeepFace\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_images_from_folder(folder):\n",
        "    known_face_encodings = []\n",
        "    known_face_names = []\n",
        "\n",
        "    # Percorrer todos os arquivos na pasta fornecida\n",
        "    for filename in os.listdir(folder):\n",
        "        # Verificar se o arquivo é uma imagem\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            # Carregar a imagem\n",
        "            image_path = os.path.join(folder, filename)\n",
        "            image = face_recognition.load_image_file(image_path)\n",
        "            # Obter as codificações faciais (assumindo uma face por imagem)\n",
        "            face_encodings = face_recognition.face_encodings(image)\n",
        "\n",
        "            if face_encodings:\n",
        "                face_encoding = face_encodings[0]\n",
        "                # Extrair o nome do arquivo, removendo o sufixo numérico e a extensão\n",
        "                name = os.path.splitext(filename)[0][:-1]\n",
        "                name = name.split(\"_\")[0]\n",
        "                # Adicionar a codificação e o nome às listas\n",
        "                known_face_encodings.append(face_encoding)\n",
        "                known_face_names.append(name)\n",
        "\n",
        "    return known_face_encodings, known_face_names\n",
        "\n",
        "\n",
        "mapa_indice_chave_mediapipe={}\n",
        "mapa_indice_chave_mediapipe[0] = 'nose'\n",
        "mapa_indice_chave_mediapipe[1] = 'left_eye_inner'\n",
        "mapa_indice_chave_mediapipe[2] = 'left_eye'\n",
        "mapa_indice_chave_mediapipe[3] = 'left_eye_outer'\n",
        "mapa_indice_chave_mediapipe[4] = 'right_eye_inner'\n",
        "mapa_indice_chave_mediapipe[5] = 'right_eye'\n",
        "mapa_indice_chave_mediapipe[6] = 'right_eye_outer'\n",
        "mapa_indice_chave_mediapipe[7] = 'left_ear'\n",
        "mapa_indice_chave_mediapipe[8] = 'right_ear'\n",
        "mapa_indice_chave_mediapipe[9] = 'mouth_left'\n",
        "mapa_indice_chave_mediapipe[10] = 'mouth_right'\n",
        "mapa_indice_chave_mediapipe[11] = 'left_shoulder'\n",
        "mapa_indice_chave_mediapipe[12] = 'right_shoulder'\n",
        "mapa_indice_chave_mediapipe[13] = 'left_elbow'\n",
        "mapa_indice_chave_mediapipe[14] = 'right_elbow'\n",
        "mapa_indice_chave_mediapipe[15] = 'left_wrist'\n",
        "mapa_indice_chave_mediapipe[16] = 'right_wrist'\n",
        "mapa_indice_chave_mediapipe[17] = 'left_pinky'\n",
        "mapa_indice_chave_mediapipe[18] = 'right_pinky'\n",
        "mapa_indice_chave_mediapipe[19] = 'left_index'\n",
        "mapa_indice_chave_mediapipe[20] = 'right_index'\n",
        "mapa_indice_chave_mediapipe[21] = 'left_thumb'\n",
        "mapa_indice_chave_mediapipe[22] = 'right_thumb'\n",
        "mapa_indice_chave_mediapipe[23] = 'left_hip'\n",
        "mapa_indice_chave_mediapipe[24] = 'right_hip'\n",
        "mapa_indice_chave_mediapipe[25] = 'left_knee'\n",
        "mapa_indice_chave_mediapipe[26] = 'right_knee'\n",
        "mapa_indice_chave_mediapipe[27] = 'left_ankle'\n",
        "mapa_indice_chave_mediapipe[28] = 'right_ankle'\n",
        "mapa_indice_chave_mediapipe[29] = 'left_heel'\n",
        "mapa_indice_chave_mediapipe[30] = 'right_heel'\n",
        "mapa_indice_chave_mediapipe[31] = 'left_foot_index'\n",
        "mapa_indice_chave_mediapipe[32] = 'right_foot_index'\n",
        "\n",
        "\n",
        "# Função para verificar movimentos\n",
        "def coleta_posicao_landmarks(results_pose, mp_pose):\n",
        "    lista_mapa_landmarks = []\n",
        "    pose_landmarks_list = results_pose.pose_landmarks\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "        landmark_list = [ landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z, visibility=landmark.visibility, presence=landmark.presence) for landmark in pose_landmarks ]\n",
        "\n",
        "        mapa_posicionamento = {}\n",
        "\n",
        "        for idx_landmark_item, landmark_item in enumerate(landmark_list):\n",
        "          mapa_posicionamento[mapa_indice_chave_mediapipe[idx_landmark_item]] = landmark_item\n",
        "\n",
        "        lista_mapa_landmarks.append(mapa_posicionamento)\n",
        "\n",
        "    return lista_mapa_landmarks\n",
        "\n",
        "\n",
        "def converter_landmarks_numpy(results_pose, width, height):\n",
        "    lista_current_landmarks = []\n",
        "    pose_landmarks_list = results_pose.pose_landmarks\n",
        "\n",
        "    for idx in range(len(pose_landmarks_list)):\n",
        "        pose_landmarks = pose_landmarks_list[idx]\n",
        "\n",
        "        current_landmarks = np.array(\n",
        "            [[lm.x * width, lm.y * height] for lm in pose_landmarks]\n",
        "        )\n",
        "        lista_current_landmarks.append(current_landmarks)\n",
        "\n",
        "    return lista_current_landmarks\n",
        "\n",
        "\n",
        "\n",
        "# Mapear partes do corpo com landmarks\n",
        "partes_corpo = {\n",
        "    \"cabeca\": [0, 1, 2, 3, 4, 5, 6, 7],  # Nariz, olhos, orelhas\n",
        "    \"braco esquerdo\": [11, 13, 15],  # Ombro, cotovelo, pulso esquerdo\n",
        "    \"braco direito\": [12, 14, 16],  # Ombro, cotovelo, pulso direito\n",
        "    \"perna esquerda\": [23, 25, 27],  # Quadril, joelho, tornozelo esquerdo\n",
        "    \"perna direita\": [24, 26, 28],  # Quadril, joelho, tornozelo direito\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "pe_esquerdo_a_frente = False\n",
        "braco_esquerdo_levantado = False\n",
        "braco_direito_levantado = False\n",
        "\n",
        "lista_posicionamento_anterior = None\n",
        "def detecta_movimentos(results_pose, mp_pose, arquivo_relatorio_saida, cv2, frame, width, height, movement_threshold=22):\n",
        "    global pe_esquerdo_a_frente\n",
        "    global braco_esquerdo_levantado\n",
        "    global braco_direito_levantado\n",
        "    global partes_corpo\n",
        "    global lista_posicionamento_anterior\n",
        "\n",
        "    lista_mapa_posicionamento = coleta_posicao_landmarks(results_pose, mp_pose)\n",
        "    lista_current_landmarks = converter_landmarks_numpy(results_pose, width, height)\n",
        "\n",
        "\n",
        "    if not lista_posicionamento_anterior:\n",
        "        lista_posicionamento_anterior = lista_current_landmarks\n",
        "\n",
        "    for idx, (landmarks_corrente, landmarks_anterior, mapa_posicionamento_corrente) in enumerate(zip(lista_current_landmarks, lista_posicionamento_anterior, lista_mapa_posicionamento)):\n",
        "        movimentos = []\n",
        "        gestos = []\n",
        "        cores = {}\n",
        "\n",
        "        # Detectar movimento por parte do corpo\n",
        "        for parte, indices in partes_corpo.items():\n",
        "            deslocamento = np.mean(np.linalg.norm(landmarks_corrente[indices] - landmarks_anterior[indices], axis=1))\n",
        "            if deslocamento > movement_threshold:\n",
        "                membros_visiveis = True\n",
        "\n",
        "                for indice in indices:\n",
        "                    if mapa_posicionamento_corrente[mapa_indice_chave_mediapipe[indice]].visibility < 0.8:\n",
        "                        membros_visiveis = False\n",
        "                        break\n",
        "\n",
        "                if membros_visiveis:\n",
        "                    movimentos.append(parte)\n",
        "\n",
        "        # Detectar gestos específicos\n",
        "        if \"braco direito\" in movimentos:\n",
        "            movimento_mao = np.linalg.norm(landmarks_corrente[16] - landmarks_corrente[12])\n",
        "            if movimento_mao > movement_threshold * 2:\n",
        "                gestos.append(\"Aceno de mao (direito)\")\n",
        "\n",
        "        if \"braco esquerdo\" in movimentos:\n",
        "            movimento_mao = np.linalg.norm(landmarks_corrente[15] - landmarks_corrente[11])\n",
        "            if movimento_mao > movement_threshold * 2:\n",
        "                gestos.append(\"Aceno de mao (esquerdo)\")\n",
        "\n",
        "        if \"perna direita\" in movimentos and \"perna esquerda\" in movimentos:\n",
        "            movimento_perna_direita = np.linalg.norm(landmarks_corrente[24] - landmarks_anterior[24])\n",
        "            movimento_perna_esquerda = np.linalg.norm(landmarks_corrente[23] - landmarks_anterior[23])\n",
        "            if abs(movimento_perna_direita - movimento_perna_esquerda) > movement_threshold:\n",
        "                gestos.append(\"Caminhando\")\n",
        "\n",
        "        if movimentos:\n",
        "            arquivo_relatorio_saida.write(f\"\\tMovimento {idx}: {movimentos}\\n\")\n",
        "\n",
        "            cv2.putText(frame, f\"Movimento {idx}: {movimentos}\", (10, 30 + (idx * 60)),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "        if gestos:\n",
        "            arquivo_relatorio_saida.write(f\"\\tGestos {idx}: {gestos}\\n\")\n",
        "            cv2.putText(frame, f\"Gestos {idx}: {gestos}\", (10, 60 + (idx * 60)),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "\n",
        "    lista_posicionamento_anterior = lista_current_landmarks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def processa_video(video_path, output_path, exibir_frame_processado=False, diretorio_projeto=\".\"):\n",
        "\n",
        "    base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n",
        "    options = vision.PoseLandmarkerOptions(\n",
        "        num_poses=10,\n",
        "        base_options=base_options,\n",
        "        output_segmentation_masks=True)\n",
        "    detector = vision.PoseLandmarker.create_from_options(options)\n",
        "\n",
        "    # Inicializar o MediaPipe Pose\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(\n",
        "        # min_detection_confidence=0.3,\n",
        "        # min_tracking_confidence=0.3\n",
        "    )\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "    # Capturar vídeo do arquivo especificado\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Verificar se o vídeo foi aberto corretamente\n",
        "    if not cap.isOpened():\n",
        "        print(\"Erro ao abrir o vídeo.\")\n",
        "        return\n",
        "\n",
        "    # Obter propriedades do vídeo\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Definir o codec e criar o objeto VideoWriter\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec para MP4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    arquivo_relatorio_saida_caminho = diretorio_projeto + \"relatorio_saida.txt\"\n",
        "\n",
        "    with open(arquivo_relatorio_saida_caminho, \"w\") as arquivo_relatorio_saida:\n",
        "\n",
        "        # Gravando o total de frames do vídeo\n",
        "        arquivo_relatorio_saida.write(\n",
        "            f\"\"\"Inicio do processamento\n",
        "  Total Frames: {total_frames}\\n\\n\"\"\")\n",
        "\n",
        "        # Loop para processar cada frame do vídeo com barra de progresso\n",
        "        for item_tqdm in tqdm(range(total_frames), desc=\"Processando vídeo\"):\n",
        "\n",
        "            # Ler um frame do vídeo\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            # Se não conseguiu ler o frame (final do vídeo), sair do loop\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Gravando o índice de frame lido\n",
        "            arquivo_relatorio_saida.write(\n",
        "                f\"\\n\\n\\t----------\\n\\tFrame: {item_tqdm}\\n\")\n",
        "\n",
        "            # Analisar o frame para detectar faces e expressões\n",
        "            result_deepFace = DeepFace.analyze(\n",
        "                frame, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "            # Converter o frame para RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Processar o frame para detectar a pose\n",
        "            # results_pose = pose.process(rgb_frame)\n",
        "            frame_to_detector = mp.Image(\n",
        "                image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "            results_pose = detector.detect(frame_to_detector)\n",
        "\n",
        "            face_locations = face_recognition.face_locations(\n",
        "                rgb_frame, model=\"cnn\")\n",
        "            face_encodings = face_recognition.face_encodings(\n",
        "                rgb_frame, face_locations)\n",
        "\n",
        "            # Inicializar uma lista de nomes para as faces detectadas\n",
        "            face_names = []\n",
        "            for face_encoding in face_encodings:\n",
        "                matches = face_recognition.compare_faces(\n",
        "                    known_face_encodings, face_encoding)\n",
        "                name = \"Desconhecido\"\n",
        "                face_distances = face_recognition.face_distance(\n",
        "                    known_face_encodings, face_encoding)\n",
        "                best_match_index = np.argmin(face_distances)\n",
        "                if matches[best_match_index]:\n",
        "                    name = known_face_names[best_match_index]\n",
        "                face_names.append(name)\n",
        "\n",
        "            # Iterar sobre cada face detectada\n",
        "            for idx_face, face in enumerate(result_deepFace):\n",
        "                # Obter a caixa delimitadora da face\n",
        "                x, y, w, h = face['region']['x'], face['region']['y'], face['region']['w'], face['region']['h']\n",
        "\n",
        "                # Obter a emoção dominante\n",
        "                dominant_emotion = face['dominant_emotion']\n",
        "\n",
        "                # Desenhar um retângulo ao redor da face\n",
        "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "                # Escrever a emoção dominante acima da face\n",
        "                cv2.putText(frame, dominant_emotion, (x-50, y + h + 60),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
        "\n",
        "                arquivo_relatorio_saida.write(\n",
        "                    f\"\\tExpressao dominante {idx_face}: {dominant_emotion}\\n\")\n",
        "\n",
        "                # Associar a face detectada pelo DeepFace com as faces conhecidas\n",
        "                for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
        "                    if x <= left <= x + w and y <= top <= y + h:\n",
        "                        # Escrever o nome abaixo da face\n",
        "                        cv2.putText(frame, name, (x - 50, y + h + 30),\n",
        "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
        "                        arquivo_relatorio_saida.write(f\"\\tFace reconhecida {idx_face}: {name} \\t posicao: (top: {top}, right: {right}, bottom: {bottom}, left: {left})\\n\")\n",
        "                        break\n",
        "\n",
        "            # Desenhar as anotações da pose no frame\n",
        "            if results_pose.pose_landmarks:\n",
        "                lista_mapa_landmarks = []\n",
        "                pose_landmarks_list = results_pose.pose_landmarks\n",
        "                for idx in range(len(pose_landmarks_list)):\n",
        "                    pose_landmarks = pose_landmarks_list[idx]\n",
        "                    # Draw the pose landmarks.\n",
        "                    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "                    pose_landmarks_proto.landmark.extend([\n",
        "                        landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
        "                    ])\n",
        "                    mp_drawing.draw_landmarks(\n",
        "                        frame, pose_landmarks_proto, mp_pose.POSE_CONNECTIONS)\n",
        "                detecta_movimentos(\n",
        "                    results_pose, mp_pose, arquivo_relatorio_saida, cv2, frame, width, height)\n",
        "\n",
        "            # Escrever o frame processado no vídeo de saída\n",
        "            out.write(frame)\n",
        "\n",
        "            if exibir_frame_processado:\n",
        "                # Exibir o frame processado\n",
        "                cv2.imshow('Video', frame)\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "                # Gravando o total de frames do vídeo\n",
        "        arquivo_relatorio_saida.write(\n",
        "            f\"\"\"\\n\\n\\nFim do processamento\n",
        "  Total Frames: {total_frames}\\n\\n\"\"\")\n",
        "\n",
        "    # Liberar a captura de vídeo e fechar todas as janelas\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    if exibir_frame_processado:\n",
        "        cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "spqBOAuUYjXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_projeto = '/content/drive/MyDrive/TI/Cursos/FIAP/IAParaDEVS/TechChallenge/Fase_04/'"
      ],
      "metadata": {
        "id": "Edw_k18coDDw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para a pasta de imagens com rostos conhecidos\n",
        "image_folder = caminho_projeto + 'images'\n",
        "\n",
        "# Carregar imagens e codificações\n",
        "known_face_encodings, known_face_names = load_images_from_folder(image_folder)"
      ],
      "metadata": {
        "id": "As_l47X5jwML"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho para o vídeo de entrada e saída\n",
        "input_video_path = os.path.join(caminho_projeto, 'Unlocking Facial Recognition_ Diverse Activities Analysis.mp4')  # Nome do vídeo de entrada\n",
        "output_video_path = os.path.join(caminho_projeto, 'output_detection_and_recognition_Unlocking Facial Recognition_ Diverse Activities Analysis.mp4')  # Nome do vídeo de saída"
      ],
      "metadata": {
        "id": "bIQ1Amytj4Ie"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processar o vídeo\n",
        "processa_video(input_video_path, output_video_path, exibir_frame_processado=False, diretorio_projeto=caminho_projeto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFiqx1TSj9-V",
        "outputId": "52ab62a2-de95-4c76-ae3b-97f5f9533311"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando vídeo: 100%|██████████| 3326/3326 [24:51<00:00,  2.23it/s]\n"
          ]
        }
      ]
    }
  ]
}